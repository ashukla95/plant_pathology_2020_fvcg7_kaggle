{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import efficientnet.tfkeras as efn\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 80\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_path(data):\n",
    "    return \"images/{}.jpg\".format(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_paths = train_df.image_id.apply(format_path).values\n",
    "test_paths = test_df.image_id.apply(format_path).values\n",
    "train_labels = train_df.loc[:, \"healthy\":].values\n",
    "train_paths, valid_paths, train_labels, valid_labels = train_test_split(\n",
    "    train_paths, train_labels, test_size=0.2, random_state=18\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(filename, label=None, image_size=(IMAGE_SIZE, IMAGE_SIZE)):\n",
    "    bits = tf.io.read_file(filename)\n",
    "    image = tf.io.decode_jpeg(bits, channels=3)\n",
    "    image = tf.cast(image, tf.float32)/255.0\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    if label is None:\n",
    "        return image\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(image, label=None):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    if label is None:\n",
    "        return image\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((train_paths, train_labels))\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .cache()\n",
    "    .map(data_augment, num_parallel_calls=AUTO)\n",
    "    .repeat()\n",
    "    .shuffle(256)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((valid_paths, valid_labels))\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((test_paths))\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_p = ModelCheckpoint(filepath=\"model_ef.h5\", monitor='val_loss', save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    efn.EfficientNetB7(\n",
    "        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False\n",
    "    ),\n",
    "    L.GlobalAveragePooling2D(),\n",
    "    L.Dense(train_labels.shape[1], activation=\"softmax\")\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"categorical_accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = train_labels.shape[0]//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[ch_p],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=valid_dataset\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "        tf.keras.applications.DenseNet121(\n",
    "                input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "            weights='imagenet',\n",
    "            include_top=False\n",
    "        ),\n",
    "        L.GlobalAveragePooling2D(),\n",
    "        L.Dense(train_labels.shape[1], activation='softmax')\n",
    "    ])\n",
    "model2.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"categorical_accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chp_den121 = ModelCheckpoint(filepath=\"model_den.h5\", monitor=\"val_loss\", save_weights_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_den = model2.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[chp_den121],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op =(model.predict(test_dataset)+model2.predict(test_dataset))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "sub.loc[:, 'healthy':] = op\n",
    "sub.to_csv('submission_ef7_den121.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.Sequential([\n",
    "    tf.keras.applications.DenseNet201(\n",
    "        input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False\n",
    "    ),\n",
    "    L.GlobalAveragePooling2D(),\n",
    "    L.Dense(train_labels.shape[1], activation=\"softmax\")\n",
    "])\n",
    "model3.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"categorical_accuracy\"]\n",
    ")\n",
    "chp_den201 = ModelCheckpoint(filepath=\"model_den201.h5\", monitor=\"val_loss\", save_weights_only=True, verbose=1)\n",
    "history_den = model3.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[chp_den121],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=valid_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op =(model.predict(test_dataset)+model2.predict(test_dataset)+model3.predict(test_dataset))/3\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "import datetime\n",
    "sub.loc[:, 'healthy':] = op\n",
    "sub.to_csv('submission_ef7_den121_den201_id.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op =(model.predict(test_dataset)+model3.predict(test_dataset))/2\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "import datetime\n",
    "sub.loc[:, 'healthy':] = op\n",
    "sub.to_csv('submission_ef7_den201_id.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
